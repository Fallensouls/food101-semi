11/09/2020 15:31:58 - INFO - __main__ -   Total params: 0.48M
11/09/2020 15:31:59 - INFO - __main__ -   ***** Running training *****
11/09/2020 15:31:59 - INFO - __main__ -     Task = food101@4000
11/09/2020 15:31:59 - INFO - __main__ -     Num Epochs = 256
11/09/2020 15:31:59 - INFO - __main__ -     Batch size per GPU = 48
11/09/2020 15:31:59 - INFO - __main__ -     Total train batch size = 48
11/09/2020 15:31:59 - INFO - __main__ -     Total optimization steps = 262144
  0%|                                                                                                                                                               | 0/1024 [00:00<?, ?it/s]/home/hatsunemiku/.local/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Train Epoch: 1/ 256. Iter:   20/1024. LR: 0.0600. Loss: 4.6451. Loss_x: 4.6451. Loss_u: 0.0000. Mask: 0.00. :   2%|▊                                       | 20/1024 [00:12<07:34,  2.21it/s]^CTraceback (most recent call last):
  File "byol.py", line 682, in <module>
    main()
  File "byol.py", line 354, in main
    model, ema_model, scheduler, writer)
  File "byol.py", line 541, in train_linear
    losses.update(loss.item())
KeyboardInterrupt
Train Epoch: 1/ 256. Iter:   20/1024. LR: 0.0600. Loss: 4.6451. Loss_x: 4.6451. Loss_u: 0.0000. Mask: 0.00. :   2%|▊                                       | 20/1024 [00:13<11:25,  1.46it/s]
➜  food101-semi git:(main) ✗ python byol.py --dataset food101 --num-labeled 4000 --arch wideresnet --batch-size 48 --lr 0.06 --out results/food@0.2b --per-labeled 0.2

/usr/local/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/usr/local/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/usr/local/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/usr/local/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/usr/local/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/usr/local/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
11/09/2020 15:32:25 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
11/09/2020 15:32:25 - INFO - __main__ -   {'T': 1, 'amp': False, 'arch': 'wideresnet', 'batch_size': 48, 'dataset': 'food101', 'device': device(type='cuda', index=0), 'ema_decay': 0.999, 'eval_steps': 1024, 'gpu_id': 0, 'k_img': 65536, 'lambda_u': 1, 'local_rank': -1, 'lr': 0.06, 'mu': 4, 'n_gpu': 1, 'nesterov': True, 'no_progress': False, 'num_labeled': 4000, 'num_workers': 4, 'opt_level': 'O1', 'out': 'results/food@0.2b', 'per_labeled': 0.2, 'resume': '', 'seed': 5, 'start_epoch': 0, 'threshold': 0.95, 'total_steps': 262144, 'use_ema': True, 'warmup': 0, 'wdecay': 0.0005, 'world_size': 1}
11/09/2020 15:32:25 - INFO - dataset.cifar -   Dataset: food101
11/09/2020 15:32:25 - INFO - models.wideresnet -   Model: WideResNet 28x1
11/09/2020 15:32:25 - INFO - __main__ -   Total params: 0.48M
11/09/2020 15:32:26 - INFO - __main__ -   ***** Running training *****
11/09/2020 15:32:26 - INFO - __main__ -     Task = food101@4000
11/09/2020 15:32:26 - INFO - __main__ -     Num Epochs = 256
11/09/2020 15:32:26 - INFO - __main__ -     Batch size per GPU = 48
11/09/2020 15:32:26 - INFO - __main__ -     Total train batch size = 48
11/09/2020 15:32:26 - INFO - __main__ -     Total optimization steps = 262144
Train Epoch: 1/ 256. Iter: 1024/1024. LR: 0.0600. Loss: 0.7187. Loss_b: 0.7187. : 100%|██████████████████████████████████████████████████████████████████| 1024/1024 [09:30<00:00,  1.80it/s]
Train Epoch: 1/ 256. Iter: 1024/1024. LR: 0.0600. Loss: 4.4663. Loss_x: 4.4663. Loss_u: 0.0000. Mask: 0.00. : 100%|██████████████████████████████████████| 1024/1024 [09:42<00:00,  1.76it/s]
Test Iter:  527/ 527. Data: 0.045s. Batch: 0.056s. Loss: 4.6098. top1: 1.20. top5: 6.21. : 100%|███████████████████████████████████████████████████████████| 527/527 [00:29<00:00, 17.86it/s]
11/09/2020 15:52:10 - INFO - __main__ -   top-1 acc: 1.20
11/09/2020 15:52:10 - INFO - __main__ -   top-5 acc: 6.21
11/09/2020 15:52:11 - INFO - __main__ -   Best top-1 acc: 1.20
11/09/2020 15:52:11 - INFO - __main__ -   Mean top-1 acc: 1.20

Train Epoch: 2/ 256. Iter:  482/1024. LR: 0.0600. Loss: 4.3980. Loss_x: 4.3979. Loss_u: 0.0001. Mask: 0.00. :  47%|██████████████████▎                    | 482/1024 [04:25<05:47,  1.56it/s]^CTraceback (most recent call last):
  File "byol.py", line 682, in <module>
    main()
  File "byol.py", line 354, in main
    model, ema_model, scheduler, writer)
  File "byol.py", line 541, in train_linear
    losses.update(loss.item())
KeyboardInterrupt
Train Epoch: 2/ 256. Iter:  482/1024. LR: 0.0600. Loss: 4.3980. Loss_x: 4.3979. Loss_u: 0.0001. Mask: 0.00. :  47%|██████████████████▎                    | 482/1024 [04:25<04:58,  1.82it/s]
➜  food101-semi git:(main) ✗ python train.py --dataset food101 --num-labeled 4000 --arch wideresnet --batch-size 48 --lr 0.06 --out results/food@0.2c --per-labeled 0.2

/usr/local/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/usr/local/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/usr/local/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/usr/local/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/usr/local/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/usr/local/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
11/09/2020 16:09:37 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
11/09/2020 16:09:37 - INFO - __main__ -   {'T': 1, 'amp': False, 'arch': 'wideresnet', 'batch_size': 48, 'dataset': 'food101', 'device': device(type='cuda', index=0), 'ema_decay': 0.999, 'eval_steps': 1024, 'gpu_id': 0, 'k_img': 65536, 'lambda_u': 1, 'local_rank': -1, 'lr': 0.06, 'mu': 4, 'n_gpu': 1, 'nesterov': True, 'no_progress': False, 'num_labeled': 4000, 'num_workers': 4, 'opt_level': 'O1', 'out': 'results/food@0.2c', 'per_labeled': 0.2, 'resume': '', 'seed': 5, 'start_epoch': 0, 'threshold': 0.95, 'total_steps': 262144, 'use_ema': True, 'warmup': 0, 'wdecay': 0.0005, 'world_size': 1}
11/09/2020 16:09:37 - INFO - dataset.cifar -   Dataset: food101
11/09/2020 16:09:37 - INFO - models.wideresnet -   Model: WideResNet 28x2
11/09/2020 16:09:37 - INFO - __main__ -   Total params: 1.61M
11/09/2020 16:09:38 - INFO - __main__ -   ***** Running training *****
11/09/2020 16:09:38 - INFO - __main__ -     Task = food101@4000
11/09/2020 16:09:38 - INFO - __main__ -     Num Epochs = 256
11/09/2020 16:09:38 - INFO - __main__ -     Batch size per GPU = 48
11/09/2020 16:09:38 - INFO - __main__ -     Total train batch size = 48
11/09/2020 16:09:38 - INFO - __main__ -     Total optimization steps = 262144
WideResNet(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (block1): NetworkBlock(
    (layer): Sequential(
      (0): BasicBlock(
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (convShortcut): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (1): BasicBlock(
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BasicBlock(
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BasicBlock(
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (block2): NetworkBlock(
    (layer): Sequential(
      (0): BasicBlock(
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (convShortcut): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      )
      (1): BasicBlock(
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BasicBlock(
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BasicBlock(
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (block3): NetworkBlock(
    (layer): Sequential(
      (0): BasicBlock(
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (convShortcut): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
      )
      (1): BasicBlock(
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BasicBlock(
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BasicBlock(
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
  (relu): LeakyReLU(negative_slope=0.1, inplace=True)
  (fc): Linear(in_features=128, out_features=101, bias=True)
  (projetion): MLPHead(
    (net): Sequential(
      (0): Linear(in_features=128, out_features=512, bias=True)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Linear(in_features=512, out_features=128, bias=True)
    )
  )
)
Train Epoch: 1/ 256. Iter:    1/1024. LR: 0.0600. Loss: 4.6234. Loss_x: 4.6234. Loss_u: 0.0000. Mask: 0.00. :   0%|                                         | 1/1024 [00:01<32:58,  1.93s/it]^CTraceback (most recent call last):
  File "train.py", line 517, in <module>
    main()
  File "train.py", line 331, in main
    model, optimizer, ema_model, scheduler, writer)
  File "train.py", line 399, in train
    losses.update(loss.item())
KeyboardInterrupt
Train Epoch: 1/ 256. Iter:    1/1024. LR: 0.0600. Loss: 4.6234. Loss_x: 4.6234. Loss_u: 0.0000. Mask: 0.00. :   0%|                                         | 1/1024 [00:02<43:05,  2.53s/it]
➜  food101-semi git:(main) ✗ python simclr.py --dataset food101 --num-labeled 4000 --arch wideresnet --batch-size 48 --lr 0.06 --out results/food@0.2c --per-labeled 0.2

/usr/local/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/usr/local/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/usr/local/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/usr/local/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/usr/local/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/usr/local/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
11/09/2020 16:34:24 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
11/09/2020 16:34:24 - INFO - __main__ -   {'T': 1, 'amp': False, 'arch': 'wideresnet', 'batch_size': 48, 'dataset': 'food101', 'device': device(type='cuda', index=0), 'ema_decay': 0.999, 'eval_steps': 1024, 'gpu_id': 0, 'k_img': 65536, 'lambda_u': 1, 'local_rank': -1, 'lr': 0.06, 'mu': 4, 'n_gpu': 1, 'nesterov': True, 'no_progress': False, 'num_labeled': 4000, 'num_workers': 4, 'opt_level': 'O1', 'out': 'results/food@0.2c', 'per_labeled': 0.2, 'resume': '', 'seed': 5, 'start_epoch': 0, 'threshold': 0.95, 'total_steps': 262144, 'use_ema': True, 'warmup': 0, 'wdecay': 0.0005, 'world_size': 1}
11/09/2020 16:34:24 - INFO - dataset.cifar -   Dataset: food101
11/09/2020 16:34:24 - INFO - models.wideresnet -   Model: WideResNet 28x2
11/09/2020 16:34:24 - INFO - __main__ -   Total params: 1.48M
11/09/2020 16:34:26 - INFO - __main__ -   ***** Running training *****
11/09/2020 16:34:26 - INFO - __main__ -     Task = food101@4000
11/09/2020 16:34:26 - INFO - __main__ -     Num Epochs = 256
11/09/2020 16:34:26 - INFO - __main__ -     Batch size per GPU = 48
11/09/2020 16:34:26 - INFO - __main__ -     Total train batch size = 48
11/09/2020 16:34:26 - INFO - __main__ -     Total optimization steps = 262144
WideResNet(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (block1): NetworkBlock(
    (layer): Sequential(
      (0): BasicBlock(
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (convShortcut): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (1): BasicBlock(
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BasicBlock(
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BasicBlock(
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (block2): NetworkBlock(
    (layer): Sequential(
      (0): BasicBlock(
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (convShortcut): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      )
      (1): BasicBlock(
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BasicBlock(
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BasicBlock(
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (block3): NetworkBlock(
    (layer): Sequential(
      (0): BasicBlock(
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (convShortcut): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
      )
      (1): BasicBlock(
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BasicBlock(
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BasicBlock(
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
  (relu): LeakyReLU(negative_slope=0.1, inplace=True)
  (fc): Linear(in_features=128, out_features=101, bias=True)
)
  0%|                                                                                                                                                               | 0/1024 [00:00<?, ?it/s]Traceback (most recent call last):
  File "simclr.py", line 538, in <module>
    main()
  File "simclr.py", line 332, in main
    model, optimizer, ema_model, scheduler, writer)
  File "simclr.py", line 399, in train
    Ls = simclr_loss(features_u_w, features_u_s, simclr_criterion)
  File "simclr.py", line 534, in simclr_loss
    loss = criterion(zis, zjs)
  File "/home/hatsunemiku/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/hatsunemiku/dev/food101-semi/models/nt_xent.py", line 53, in forward
    positives = torch.cat([l_pos, r_pos]).view(2 * self.batch_size, 1)
RuntimeError: shape '[96, 1]' is invalid for input of size 672
  0%|                                                                                                                                                               | 0/1024 [00:01<?, ?it/s]
➜  food101-semi git:(main) ✗ python simclr.py --dataset food101 --num-labeled 4000 --arch wideresnet --batch-size 48 --lr 0.06 --out results/food@0.2c --per-labeled 0.2 --mu=1

/usr/local/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/usr/local/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/usr/local/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/usr/local/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/usr/local/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/usr/local/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
11/09/2020 16:43:12 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
11/09/2020 16:43:12 - INFO - __main__ -   {'T': 1, 'amp': False, 'arch': 'wideresnet', 'batch_size': 48, 'dataset': 'food101', 'device': device(type='cuda', index=0), 'ema_decay': 0.999, 'eval_steps': 1024, 'gpu_id': 0, 'k_img': 65536, 'lambda_u': 1, 'local_rank': -1, 'lr': 0.06, 'mu': 1, 'n_gpu': 1, 'nesterov': True, 'no_progress': False, 'num_labeled': 4000, 'num_workers': 4, 'opt_level': 'O1', 'out': 'results/food@0.2c', 'per_labeled': 0.2, 'resume': '', 'seed': 5, 'start_epoch': 0, 'threshold': 0.95, 'total_steps': 262144, 'use_ema': True, 'warmup': 0, 'wdecay': 0.0005, 'world_size': 1}
11/09/2020 16:43:12 - INFO - dataset.cifar -   Dataset: food101
11/09/2020 16:43:12 - INFO - models.wideresnet -   Model: WideResNet 28x2
11/09/2020 16:43:12 - INFO - __main__ -   Total params: 1.48M
11/09/2020 16:43:13 - INFO - __main__ -   ***** Running training *****
11/09/2020 16:43:13 - INFO - __main__ -     Task = food101@4000
11/09/2020 16:43:13 - INFO - __main__ -     Num Epochs = 256
11/09/2020 16:43:13 - INFO - __main__ -     Batch size per GPU = 48
11/09/2020 16:43:13 - INFO - __main__ -     Total train batch size = 48
11/09/2020 16:43:13 - INFO - __main__ -     Total optimization steps = 262144
WideResNet(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (block1): NetworkBlock(
    (layer): Sequential(
      (0): BasicBlock(
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (convShortcut): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (1): BasicBlock(
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BasicBlock(
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BasicBlock(
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (block2): NetworkBlock(
    (layer): Sequential(
      (0): BasicBlock(
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (convShortcut): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      )
      (1): BasicBlock(
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BasicBlock(
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BasicBlock(
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (block3): NetworkBlock(
    (layer): Sequential(
      (0): BasicBlock(
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (convShortcut): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
      )
      (1): BasicBlock(
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BasicBlock(
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BasicBlock(
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
  (relu): LeakyReLU(negative_slope=0.1, inplace=True)
  (fc): Linear(in_features=128, out_features=101, bias=True)
)
Train Epoch: 1/ 256. Iter: 1024/1024. LR: 0.0600. Loss: 7.6289. Loss_x: 4.3529. Loss_u: 0.0000. Loss_s: 3.2760. Mask: 0.00. : 100%|██████████████████████| 1024/1024 [03:27<00:00,  4.92it/s]
  0%|                                                                                                                                                                | 0/527 [00:00<?, ?it/s]Traceback (most recent call last):
  File "simclr.py", line 538, in <module>
    main()
  File "simclr.py", line 332, in main
    model, optimizer, ema_model, scheduler, writer)
  File "simclr.py", line 446, in train
    test_loss, test_acc = test(args, test_loader, test_model, epoch)
  File "simclr.py", line 501, in test
    loss = F.cross_entropy(outputs, targets)
  File "/home/hatsunemiku/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 2021, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/home/hatsunemiku/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 1317, in log_softmax
    ret = input.log_softmax(dim)
AttributeError: 'tuple' object has no attribute 'log_softmax'
  0%|                                                                                                                                                                | 0/527 [00:00<?, ?it/s]
➜  food101-semi git:(main) ✗ python simclr.py --dataset food101 --num-labeled 4000 --arch wideresnet --batch-size 48 --lr 0.06 --out results/food@0.2c --per-labeled 0.2 --mu=1

/usr/local/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/usr/local/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/usr/local/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/usr/local/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/usr/local/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/usr/local/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
11/09/2020 16:47:11 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
11/09/2020 16:47:11 - INFO - __main__ -   {'T': 1, 'amp': False, 'arch': 'wideresnet', 'batch_size': 48, 'dataset': 'food101', 'device': device(type='cuda', index=0), 'ema_decay': 0.999, 'eval_steps': 1024, 'gpu_id': 0, 'k_img': 65536, 'lambda_u': 1, 'local_rank': -1, 'lr': 0.06, 'mu': 1, 'n_gpu': 1, 'nesterov': True, 'no_progress': False, 'num_labeled': 4000, 'num_workers': 4, 'opt_level': 'O1', 'out': 'results/food@0.2c', 'per_labeled': 0.2, 'resume': '', 'seed': 5, 'start_epoch': 0, 'threshold': 0.95, 'total_steps': 262144, 'use_ema': True, 'warmup': 0, 'wdecay': 0.0005, 'world_size': 1}
11/09/2020 16:47:11 - INFO - dataset.cifar -   Dataset: food101
11/09/2020 16:47:11 - INFO - models.wideresnet -   Model: WideResNet 28x2
11/09/2020 16:47:11 - INFO - __main__ -   Total params: 1.48M
11/09/2020 16:47:13 - INFO - __main__ -   ***** Running training *****
11/09/2020 16:47:13 - INFO - __main__ -     Task = food101@4000
11/09/2020 16:47:13 - INFO - __main__ -     Num Epochs = 256
11/09/2020 16:47:13 - INFO - __main__ -     Batch size per GPU = 48
11/09/2020 16:47:13 - INFO - __main__ -     Total train batch size = 48
11/09/2020 16:47:13 - INFO - __main__ -     Total optimization steps = 262144
WideResNet(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (block1): NetworkBlock(
    (layer): Sequential(
      (0): BasicBlock(
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (convShortcut): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (1): BasicBlock(
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BasicBlock(
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BasicBlock(
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (block2): NetworkBlock(
    (layer): Sequential(
      (0): BasicBlock(
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (convShortcut): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      )
      (1): BasicBlock(
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BasicBlock(
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BasicBlock(
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (block3): NetworkBlock(
    (layer): Sequential(
      (0): BasicBlock(
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (convShortcut): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
      )
      (1): BasicBlock(
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BasicBlock(
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BasicBlock(
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
  (relu): LeakyReLU(negative_slope=0.1, inplace=True)
  (fc): Linear(in_features=128, out_features=101, bias=True)
)
Train Epoch: 1/ 256. Iter: 1024/1024. LR: 0.0600. Loss: 7.6032. Loss_x: 4.3347. Loss_u: 0.0000. Loss_s: 3.2685. Mask: 0.00. : 100%|██████████████████████| 1024/1024 [03:28<00:00,  4.92it/s]
Test Iter:  527/ 527. Data: 0.035s. Batch: 0.056s. Loss: 4.4506. top1: 4.90. top5: 17.24. : 100%|██████████████████████████████████████████████████████████| 527/527 [00:29<00:00, 17.81it/s]
11/09/2020 16:51:10 - INFO - __main__ -   top-1 acc: 4.90
11/09/2020 16:51:10 - INFO - __main__ -   top-5 acc: 17.24
11/09/2020 16:51:10 - INFO - __main__ -   Best top-1 acc: 4.90
11/09/2020 16:51:10 - INFO - __main__ -   Mean top-1 acc: 4.90

Train Epoch: 2/ 256. Iter: 1024/1024. LR: 0.0600. Loss: 7.3224. Loss_x: 4.1241. Loss_u: 0.0000. Loss_s: 3.1983. Mask: 0.00. : 100%|██████████████████████| 1024/1024 [03:30<00:00,  4.87it/s]
Test Iter:  527/ 527. Data: 0.034s. Batch: 0.055s. Loss: 4.1875. top1: 8.53. top5: 24.71. : 100%|██████████████████████████████████████████████████████████| 527/527 [00:29<00:00, 18.10it/s]
11/09/2020 16:55:10 - INFO - __main__ -   top-1 acc: 8.53
11/09/2020 16:55:10 - INFO - __main__ -   top-5 acc: 24.71
11/09/2020 16:55:10 - INFO - __main__ -   Best top-1 acc: 8.53
11/09/2020 16:55:10 - INFO - __main__ -   Mean top-1 acc: 6.71

Train Epoch: 3/ 256. Iter: 1024/1024. LR: 0.0600. Loss: 7.1027. Loss_x: 3.9439. Loss_u: 0.0001. Loss_s: 3.1587. Mask: 0.00. : 100%|██████████████████████| 1024/1024 [03:28<00:00,  4.92it/s]
Test Iter:  527/ 527. Data: 0.034s. Batch: 0.054s. Loss: 3.8930. top1: 12.29. top5: 32.66. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:28<00:00, 18.34it/s]
11/09/2020 16:59:06 - INFO - __main__ -   top-1 acc: 12.29
11/09/2020 16:59:06 - INFO - __main__ -   top-5 acc: 32.66
11/09/2020 16:59:07 - INFO - __main__ -   Best top-1 acc: 12.29
11/09/2020 16:59:07 - INFO - __main__ -   Mean top-1 acc: 8.57

Train Epoch: 4/ 256. Iter: 1024/1024. LR: 0.0600. Loss: 6.9022. Loss_x: 3.7737. Loss_u: 0.0006. Loss_s: 3.1280. Mask: 0.00. : 100%|██████████████████████| 1024/1024 [03:28<00:00,  4.92it/s]
Test Iter:  527/ 527. Data: 0.034s. Batch: 0.055s. Loss: 3.4561. top1: 19.97. top5: 45.66. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:29<00:00, 18.04it/s]
11/09/2020 17:03:04 - INFO - __main__ -   top-1 acc: 19.97
11/09/2020 17:03:04 - INFO - __main__ -   top-5 acc: 45.66
11/09/2020 17:03:04 - INFO - __main__ -   Best top-1 acc: 19.97
11/09/2020 17:03:04 - INFO - __main__ -   Mean top-1 acc: 11.42

Train Epoch: 5/ 256. Iter: 1024/1024. LR: 0.0600. Loss: 6.7239. Loss_x: 3.6167. Loss_u: 0.0015. Loss_s: 3.1056. Mask: 0.00. : 100%|██████████████████████| 1024/1024 [03:27<00:00,  4.92it/s]
Test Iter:  527/ 527. Data: 0.034s. Batch: 0.055s. Loss: 3.0368. top1: 27.73. top5: 56.39. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:29<00:00, 18.04it/s]
11/09/2020 17:07:01 - INFO - __main__ -   top-1 acc: 27.73
11/09/2020 17:07:01 - INFO - __main__ -   top-5 acc: 56.39
11/09/2020 17:07:01 - INFO - __main__ -   Best top-1 acc: 27.73
11/09/2020 17:07:01 - INFO - __main__ -   Mean top-1 acc: 14.68

Train Epoch: 6/ 256. Iter: 1024/1024. LR: 0.0600. Loss: 6.5598. Loss_x: 3.4690. Loss_u: 0.0025. Loss_s: 3.0883. Mask: 0.02. : 100%|██████████████████████| 1024/1024 [03:27<00:00,  4.94it/s]
Test Iter:  527/ 527. Data: 0.034s. Batch: 0.055s. Loss: 2.7109. top1: 34.21. top5: 63.85. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:29<00:00, 18.14it/s]
11/09/2020 17:10:58 - INFO - __main__ -   top-1 acc: 34.21
11/09/2020 17:10:58 - INFO - __main__ -   top-5 acc: 63.85
11/09/2020 17:10:58 - INFO - __main__ -   Best top-1 acc: 34.21
11/09/2020 17:10:58 - INFO - __main__ -   Mean top-1 acc: 17.94

Train Epoch: 7/ 256. Iter: 1024/1024. LR: 0.0600. Loss: 6.4121. Loss_x: 3.3341. Loss_u: 0.0040. Loss_s: 3.0740. Mask: 0.04. : 100%|██████████████████████| 1024/1024 [03:28<00:00,  4.91it/s]
Test Iter:  527/ 527. Data: 0.034s. Batch: 0.055s. Loss: 2.4687. top1: 38.97. top5: 68.76. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:29<00:00, 18.16it/s]
11/09/2020 17:14:55 - INFO - __main__ -   top-1 acc: 38.97
11/09/2020 17:14:55 - INFO - __main__ -   top-5 acc: 68.76
11/09/2020 17:14:56 - INFO - __main__ -   Best top-1 acc: 38.97
11/09/2020 17:14:56 - INFO - __main__ -   Mean top-1 acc: 20.94

Train Epoch: 8/ 256. Iter: 1024/1024. LR: 0.0599. Loss: 6.2788. Loss_x: 3.2102. Loss_u: 0.0057. Loss_s: 3.0630. Mask: 0.12. : 100%|██████████████████████| 1024/1024 [03:28<00:00,  4.92it/s]
Test Iter:  527/ 527. Data: 0.034s. Batch: 0.055s. Loss: 2.2528. top1: 43.63. top5: 72.56. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:28<00:00, 18.19it/s]
11/09/2020 17:18:53 - INFO - __main__ -   top-1 acc: 43.63
11/09/2020 17:18:53 - INFO - __main__ -   top-5 acc: 72.56
11/09/2020 17:18:53 - INFO - __main__ -   Best top-1 acc: 43.63
11/09/2020 17:18:53 - INFO - __main__ -   Mean top-1 acc: 23.78

Train Epoch: 9/ 256. Iter: 1024/1024. LR: 0.0599. Loss: 6.1603. Loss_x: 3.0992. Loss_u: 0.0076. Loss_s: 3.0535. Mask: 0.06. : 100%|██████████████████████| 1024/1024 [03:28<00:00,  4.92it/s]
Test Iter:  527/ 527. Data: 0.034s. Batch: 0.055s. Loss: 2.0935. top1: 46.95. top5: 75.45. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:29<00:00, 18.11it/s]
11/09/2020 17:22:50 - INFO - __main__ -   top-1 acc: 46.95
11/09/2020 17:22:50 - INFO - __main__ -   top-5 acc: 75.45
11/09/2020 17:22:50 - INFO - __main__ -   Best top-1 acc: 46.95
11/09/2020 17:22:50 - INFO - __main__ -   Mean top-1 acc: 26.36

Train Epoch: 10/ 256. Iter: 1024/1024. LR: 0.0599. Loss: 6.0546. Loss_x: 2.9994. Loss_u: 0.0095. Loss_s: 3.0457. Mask: 0.08. : 100%|█████████████████████| 1024/1024 [03:27<00:00,  4.92it/s]
Test Iter:  527/ 527. Data: 0.034s. Batch: 0.055s. Loss: 1.9805. top1: 49.55. top5: 77.52. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:29<00:00, 18.12it/s]
11/09/2020 17:26:47 - INFO - __main__ -   top-1 acc: 49.55
11/09/2020 17:26:47 - INFO - __main__ -   top-5 acc: 77.52
11/09/2020 17:26:48 - INFO - __main__ -   Best top-1 acc: 49.55
11/09/2020 17:26:48 - INFO - __main__ -   Mean top-1 acc: 28.67

Train Epoch: 11/ 256. Iter: 1024/1024. LR: 0.0599. Loss: 5.9585. Loss_x: 2.9082. Loss_u: 0.0116. Loss_s: 3.0387. Mask: 0.00. : 100%|█████████████████████| 1024/1024 [03:27<00:00,  4.93it/s]
Test Iter:  527/ 527. Data: 0.034s. Batch: 0.055s. Loss: 1.8945. top1: 51.54. top5: 78.74. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:29<00:00, 18.12it/s]
11/09/2020 17:30:44 - INFO - __main__ -   top-1 acc: 51.54
11/09/2020 17:30:44 - INFO - __main__ -   top-5 acc: 78.74
11/09/2020 17:30:45 - INFO - __main__ -   Best top-1 acc: 51.54
11/09/2020 17:30:45 - INFO - __main__ -   Mean top-1 acc: 30.75

Train Epoch: 12/ 256. Iter: 1024/1024. LR: 0.0599. Loss: 5.8719. Loss_x: 2.8249. Loss_u: 0.0139. Loss_s: 3.0330. Mask: 0.04. : 100%|█████████████████████| 1024/1024 [03:28<00:00,  4.92it/s]
Test Iter:  527/ 527. Data: 0.033s. Batch: 0.055s. Loss: 1.8307. top1: 52.91. top5: 79.83. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:28<00:00, 18.29it/s]
11/09/2020 17:34:41 - INFO - __main__ -   top-1 acc: 52.91
11/09/2020 17:34:41 - INFO - __main__ -   top-5 acc: 79.83
11/09/2020 17:34:42 - INFO - __main__ -   Best top-1 acc: 52.91
11/09/2020 17:34:42 - INFO - __main__ -   Mean top-1 acc: 32.60

Train Epoch: 13/ 256. Iter: 1024/1024. LR: 0.0599. Loss: 5.7943. Loss_x: 2.7508. Loss_u: 0.0158. Loss_s: 3.0277. Mask: 0.17. : 100%|█████████████████████| 1024/1024 [03:28<00:00,  4.92it/s]
Test Iter:  527/ 527. Data: 0.034s. Batch: 0.055s. Loss: 1.7780. top1: 54.01. top5: 80.77. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:28<00:00, 18.17it/s]
11/09/2020 17:38:39 - INFO - __main__ -   top-1 acc: 54.01
11/09/2020 17:38:39 - INFO - __main__ -   top-5 acc: 80.77
11/09/2020 17:38:39 - INFO - __main__ -   Best top-1 acc: 54.01
11/09/2020 17:38:39 - INFO - __main__ -   Mean top-1 acc: 34.25

Train Epoch: 14/ 256. Iter: 1024/1024. LR: 0.0598. Loss: 5.7235. Loss_x: 2.6827. Loss_u: 0.0177. Loss_s: 3.0232. Mask: 0.15. : 100%|█████████████████████| 1024/1024 [03:28<00:00,  4.91it/s]
Test Iter:  527/ 527. Data: 0.034s. Batch: 0.055s. Loss: 1.7369. top1: 54.82. top5: 81.50. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:29<00:00, 18.06it/s]
11/09/2020 17:42:37 - INFO - __main__ -   top-1 acc: 54.82
11/09/2020 17:42:37 - INFO - __main__ -   top-5 acc: 81.50
11/09/2020 17:42:37 - INFO - __main__ -   Best top-1 acc: 54.82
11/09/2020 17:42:37 - INFO - __main__ -   Mean top-1 acc: 35.72

Train Epoch: 15/ 256. Iter: 1024/1024. LR: 0.0598. Loss: 5.6580. Loss_x: 2.6193. Loss_u: 0.0195. Loss_s: 3.0191. Mask: 0.04. : 100%|█████████████████████| 1024/1024 [03:30<00:00,  4.87it/s]
Test Iter:  527/ 527. Data: 0.035s. Batch: 0.056s. Loss: 1.7088. top1: 55.73. top5: 81.99. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:29<00:00, 17.76it/s]
11/09/2020 17:46:36 - INFO - __main__ -   top-1 acc: 55.73
11/09/2020 17:46:36 - INFO - __main__ -   top-5 acc: 81.99
11/09/2020 17:46:37 - INFO - __main__ -   Best top-1 acc: 55.73
11/09/2020 17:46:37 - INFO - __main__ -   Mean top-1 acc: 37.05

Train Epoch: 16/ 256. Iter: 1024/1024. LR: 0.0598. Loss: 5.5982. Loss_x: 2.5615. Loss_u: 0.0213. Loss_s: 3.0154. Mask: 0.10. : 100%|█████████████████████| 1024/1024 [03:27<00:00,  4.93it/s]
Test Iter:  527/ 527. Data: 0.034s. Batch: 0.055s. Loss: 1.6786. top1: 56.69. top5: 82.49. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:29<00:00, 18.03it/s]
11/09/2020 17:50:33 - INFO - __main__ -   top-1 acc: 56.69
11/09/2020 17:50:33 - INFO - __main__ -   top-5 acc: 82.49
11/09/2020 17:50:34 - INFO - __main__ -   Best top-1 acc: 56.69
11/09/2020 17:50:34 - INFO - __main__ -   Mean top-1 acc: 38.28

Train Epoch: 17/ 256. Iter: 1024/1024. LR: 0.0598. Loss: 5.5437. Loss_x: 2.5085. Loss_u: 0.0231. Loss_s: 3.0122. Mask: 0.02. : 100%|█████████████████████| 1024/1024 [03:29<00:00,  4.88it/s]
Test Iter:  527/ 527. Data: 0.034s. Batch: 0.055s. Loss: 1.6555. top1: 57.22. top5: 82.82. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:29<00:00, 18.02it/s]
11/09/2020 17:54:33 - INFO - __main__ -   top-1 acc: 57.22
11/09/2020 17:54:33 - INFO - __main__ -   top-5 acc: 82.82
11/09/2020 17:54:33 - INFO - __main__ -   Best top-1 acc: 57.22
11/09/2020 17:54:33 - INFO - __main__ -   Mean top-1 acc: 39.39

Train Epoch: 18/ 256. Iter: 1024/1024. LR: 0.0597. Loss: 5.4921. Loss_x: 2.4582. Loss_u: 0.0247. Loss_s: 3.0091. Mask: 0.10. : 100%|█████████████████████| 1024/1024 [03:30<00:00,  4.87it/s]
Test Iter:  527/ 527. Data: 0.034s. Batch: 0.056s. Loss: 1.6419. top1: 57.63. top5: 82.98. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:29<00:00, 17.96it/s]
11/09/2020 17:58:32 - INFO - __main__ -   top-1 acc: 57.63
11/09/2020 17:58:32 - INFO - __main__ -   top-5 acc: 82.98
11/09/2020 17:58:32 - INFO - __main__ -   Best top-1 acc: 57.63
11/09/2020 17:58:32 - INFO - __main__ -   Mean top-1 acc: 40.40

Train Epoch: 19/ 256. Iter: 1024/1024. LR: 0.0597. Loss: 5.4454. Loss_x: 2.4123. Loss_u: 0.0265. Loss_s: 3.0066. Mask: 0.08. : 100%|█████████████████████| 1024/1024 [03:29<00:00,  4.88it/s]
Test Iter:  527/ 527. Data: 0.034s. Batch: 0.056s. Loss: 1.6318. top1: 58.19. top5: 83.23. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:29<00:00, 17.98it/s]
11/09/2020 18:02:31 - INFO - __main__ -   top-1 acc: 58.19
11/09/2020 18:02:31 - INFO - __main__ -   top-5 acc: 83.23
11/09/2020 18:02:32 - INFO - __main__ -   Best top-1 acc: 58.19
11/09/2020 18:02:32 - INFO - __main__ -   Mean top-1 acc: 41.34

Train Epoch: 20/ 256. Iter: 1024/1024. LR: 0.0597. Loss: 5.4017. Loss_x: 2.3693. Loss_u: 0.0282. Loss_s: 3.0042. Mask: 0.12. : 100%|█████████████████████| 1024/1024 [03:29<00:00,  4.88it/s]
Test Iter:  527/ 527. Data: 0.034s. Batch: 0.056s. Loss: 1.6185. top1: 58.25. top5: 83.39. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:29<00:00, 17.98it/s]
11/09/2020 18:06:31 - INFO - __main__ -   top-1 acc: 58.25
11/09/2020 18:06:31 - INFO - __main__ -   top-5 acc: 83.39
11/09/2020 18:06:31 - INFO - __main__ -   Best top-1 acc: 58.25
11/09/2020 18:06:31 - INFO - __main__ -   Mean top-1 acc: 42.19

Train Epoch: 21/ 256. Iter: 1024/1024. LR: 0.0596. Loss: 5.3601. Loss_x: 2.3287. Loss_u: 0.0295. Loss_s: 3.0019. Mask: 0.10. : 100%|█████████████████████| 1024/1024 [03:31<00:00,  4.85it/s]
Test Iter:  527/ 527. Data: 0.035s. Batch: 0.057s. Loss: 1.6058. top1: 58.60. top5: 83.43. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:29<00:00, 17.66it/s]
11/09/2020 18:10:32 - INFO - __main__ -   top-1 acc: 58.60
11/09/2020 18:10:32 - INFO - __main__ -   top-5 acc: 83.43
11/09/2020 18:10:32 - INFO - __main__ -   Best top-1 acc: 58.60
11/09/2020 18:10:32 - INFO - __main__ -   Mean top-1 acc: 44.87

Train Epoch: 22/ 256. Iter: 1024/1024. LR: 0.0596. Loss: 5.3216. Loss_x: 2.2910. Loss_u: 0.0308. Loss_s: 2.9999. Mask: 0.12. : 100%|█████████████████████| 1024/1024 [03:29<00:00,  4.89it/s]
Test Iter:  527/ 527. Data: 0.035s. Batch: 0.056s. Loss: 1.6051. top1: 58.87. top5: 83.58. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:29<00:00, 17.89it/s]
11/09/2020 18:14:31 - INFO - __main__ -   top-1 acc: 58.87
11/09/2020 18:14:31 - INFO - __main__ -   top-5 acc: 83.58
11/09/2020 18:14:31 - INFO - __main__ -   Best top-1 acc: 58.87
11/09/2020 18:14:31 - INFO - __main__ -   Mean top-1 acc: 47.39

Train Epoch: 23/ 256. Iter: 1024/1024. LR: 0.0595. Loss: 5.2861. Loss_x: 2.2558. Loss_u: 0.0323. Loss_s: 2.9980. Mask: 0.17. : 100%|█████████████████████| 1024/1024 [03:29<00:00,  4.89it/s]
Test Iter:  527/ 527. Data: 0.035s. Batch: 0.056s. Loss: 1.5892. top1: 59.24. top5: 83.78. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:29<00:00, 17.84it/s]
11/09/2020 18:18:30 - INFO - __main__ -   top-1 acc: 59.24
11/09/2020 18:18:30 - INFO - __main__ -   top-5 acc: 83.78
11/09/2020 18:18:30 - INFO - __main__ -   Best top-1 acc: 59.24
11/09/2020 18:18:30 - INFO - __main__ -   Mean top-1 acc: 49.74

Train Epoch: 24/ 256. Iter: 1024/1024. LR: 0.0595. Loss: 5.2525. Loss_x: 2.2226. Loss_u: 0.0336. Loss_s: 2.9963. Mask: 0.08. : 100%|█████████████████████| 1024/1024 [03:29<00:00,  4.89it/s]
Test Iter:  527/ 527. Data: 0.035s. Batch: 0.056s. Loss: 1.5832. top1: 59.52. top5: 83.93. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:29<00:00, 17.91it/s]
11/09/2020 18:22:29 - INFO - __main__ -   top-1 acc: 59.52
11/09/2020 18:22:29 - INFO - __main__ -   top-5 acc: 83.93
11/09/2020 18:22:30 - INFO - __main__ -   Best top-1 acc: 59.52
11/09/2020 18:22:30 - INFO - __main__ -   Mean top-1 acc: 51.71

Train Epoch: 25/ 256. Iter: 1024/1024. LR: 0.0595. Loss: 5.2200. Loss_x: 2.1905. Loss_u: 0.0349. Loss_s: 2.9945. Mask: 0.19. : 100%|█████████████████████| 1024/1024 [03:29<00:00,  4.89it/s]
Test Iter:  527/ 527. Data: 0.035s. Batch: 0.056s. Loss: 1.5814. top1: 59.85. top5: 84.01. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:29<00:00, 17.78it/s]
11/09/2020 18:26:28 - INFO - __main__ -   top-1 acc: 59.85
11/09/2020 18:26:28 - INFO - __main__ -   top-5 acc: 84.01
11/09/2020 18:26:29 - INFO - __main__ -   Best top-1 acc: 59.85
11/09/2020 18:26:29 - INFO - __main__ -   Mean top-1 acc: 53.32

Train Epoch: 26/ 256. Iter: 1024/1024. LR: 0.0594. Loss: 5.1901. Loss_x: 2.1610. Loss_u: 0.0361. Loss_s: 2.9931. Mask: 0.19. : 100%|█████████████████████| 1024/1024 [03:29<00:00,  4.89it/s]
Test Iter:  527/ 527. Data: 0.034s. Batch: 0.056s. Loss: 1.5780. top1: 59.47. top5: 84.30. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:29<00:00, 17.95it/s]
11/09/2020 18:30:27 - INFO - __main__ -   top-1 acc: 59.47
11/09/2020 18:30:27 - INFO - __main__ -   top-5 acc: 84.30
11/09/2020 18:30:28 - INFO - __main__ -   Best top-1 acc: 59.85
11/09/2020 18:30:28 - INFO - __main__ -   Mean top-1 acc: 54.58

Train Epoch: 27/ 256. Iter: 1024/1024. LR: 0.0594. Loss: 5.1618. Loss_x: 2.1328. Loss_u: 0.0373. Loss_s: 2.9917. Mask: 0.10. : 100%|█████████████████████| 1024/1024 [03:30<00:00,  4.87it/s]
Test Iter:  527/ 527. Data: 0.037s. Batch: 0.059s. Loss: 1.5738. top1: 59.87. top5: 84.15. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:31<00:00, 16.92it/s]
11/09/2020 18:34:29 - INFO - __main__ -   top-1 acc: 59.87
11/09/2020 18:34:29 - INFO - __main__ -   top-5 acc: 84.15
11/09/2020 18:34:29 - INFO - __main__ -   Best top-1 acc: 59.87
11/09/2020 18:34:29 - INFO - __main__ -   Mean top-1 acc: 55.63

Train Epoch: 28/ 256. Iter: 1024/1024. LR: 0.0593. Loss: 5.1347. Loss_x: 2.1058. Loss_u: 0.0385. Loss_s: 2.9904. Mask: 0.19. : 100%|█████████████████████| 1024/1024 [03:46<00:00,  4.52it/s]
Test Iter:  527/ 527. Data: 0.038s. Batch: 0.059s. Loss: 1.5680. top1: 60.08. top5: 84.34. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:31<00:00, 16.86it/s]
11/09/2020 18:38:47 - INFO - __main__ -   top-1 acc: 60.08
11/09/2020 18:38:47 - INFO - __main__ -   top-5 acc: 84.34
11/09/2020 18:38:47 - INFO - __main__ -   Best top-1 acc: 60.08
11/09/2020 18:38:47 - INFO - __main__ -   Mean top-1 acc: 56.45

Train Epoch: 29/ 256. Iter: 1024/1024. LR: 0.0593. Loss: 5.1087. Loss_x: 2.0801. Loss_u: 0.0395. Loss_s: 2.9891. Mask: 0.10. : 100%|█████████████████████| 1024/1024 [03:37<00:00,  4.72it/s]
Test Iter:  527/ 527. Data: 0.036s. Batch: 0.057s. Loss: 1.5694. top1: 59.84. top5: 84.23. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:29<00:00, 17.62it/s]
11/09/2020 18:42:54 - INFO - __main__ -   top-1 acc: 59.84
11/09/2020 18:42:54 - INFO - __main__ -   top-5 acc: 84.23
11/09/2020 18:42:54 - INFO - __main__ -   Best top-1 acc: 60.08
11/09/2020 18:42:54 - INFO - __main__ -   Mean top-1 acc: 57.09

Train Epoch: 30/ 256. Iter: 1024/1024. LR: 0.0592. Loss: 5.0846. Loss_x: 2.0560. Loss_u: 0.0406. Loss_s: 2.9879. Mask: 0.17. : 100%|█████████████████████| 1024/1024 [03:30<00:00,  4.87it/s]
Test Iter:  527/ 527. Data: 0.035s. Batch: 0.056s. Loss: 1.5677. top1: 60.14. top5: 84.50. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:29<00:00, 17.74it/s]
11/09/2020 18:46:54 - INFO - __main__ -   top-1 acc: 60.14
11/09/2020 18:46:54 - INFO - __main__ -   top-5 acc: 84.50
11/09/2020 18:46:54 - INFO - __main__ -   Best top-1 acc: 60.14
11/09/2020 18:46:54 - INFO - __main__ -   Mean top-1 acc: 57.62

Train Epoch: 31/ 256. Iter: 1024/1024. LR: 0.0592. Loss: 5.0611. Loss_x: 2.0327. Loss_u: 0.0415. Loss_s: 2.9868. Mask: 0.21. : 100%|█████████████████████| 1024/1024 [03:32<00:00,  4.81it/s]
Test Iter:  527/ 527. Data: 0.039s. Batch: 0.061s. Loss: 1.5688. top1: 59.81. top5: 84.51. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:32<00:00, 16.45it/s]
11/09/2020 18:50:59 - INFO - __main__ -   top-1 acc: 59.81
11/09/2020 18:50:59 - INFO - __main__ -   top-5 acc: 84.51
11/09/2020 18:50:59 - INFO - __main__ -   Best top-1 acc: 60.14
11/09/2020 18:50:59 - INFO - __main__ -   Mean top-1 acc: 58.04

Train Epoch: 32/ 256. Iter: 1024/1024. LR: 0.0591. Loss: 5.0383. Loss_x: 2.0101. Loss_u: 0.0424. Loss_s: 2.9858. Mask: 0.10. : 100%|█████████████████████| 1024/1024 [03:35<00:00,  4.74it/s]
Test Iter:  527/ 527. Data: 0.036s. Batch: 0.057s. Loss: 1.5679. top1: 60.18. top5: 84.57. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:30<00:00, 17.51it/s]
11/09/2020 18:55:05 - INFO - __main__ -   top-1 acc: 60.18
11/09/2020 18:55:05 - INFO - __main__ -   top-5 acc: 84.57
11/09/2020 18:55:05 - INFO - __main__ -   Best top-1 acc: 60.18
11/09/2020 18:55:05 - INFO - __main__ -   Mean top-1 acc: 58.40

Train Epoch: 33/ 256. Iter: 1024/1024. LR: 0.0591. Loss: 5.0172. Loss_x: 1.9890. Loss_u: 0.0434. Loss_s: 2.9848. Mask: 0.10. : 100%|█████████████████████| 1024/1024 [03:30<00:00,  4.86it/s]
Test Iter:  527/ 527. Data: 0.034s. Batch: 0.054s. Loss: 1.5644. top1: 60.34. top5: 84.42. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:28<00:00, 18.51it/s]
11/09/2020 18:59:05 - INFO - __main__ -   top-1 acc: 60.34
11/09/2020 18:59:05 - INFO - __main__ -   top-5 acc: 84.42
11/09/2020 18:59:05 - INFO - __main__ -   Best top-1 acc: 60.34
11/09/2020 18:59:05 - INFO - __main__ -   Mean top-1 acc: 58.72

Train Epoch: 34/ 256. Iter: 1024/1024. LR: 0.0590. Loss: 4.9973. Loss_x: 1.9692. Loss_u: 0.0444. Loss_s: 2.9838. Mask: 0.10. : 100%|█████████████████████| 1024/1024 [03:31<00:00,  4.84it/s]
Test Iter:  527/ 527. Data: 0.035s. Batch: 0.056s. Loss: 1.5634. top1: 60.46. top5: 84.65. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:29<00:00, 17.92it/s]
11/09/2020 19:03:06 - INFO - __main__ -   top-1 acc: 60.46
11/09/2020 19:03:06 - INFO - __main__ -   top-5 acc: 84.65
11/09/2020 19:03:06 - INFO - __main__ -   Best top-1 acc: 60.46
11/09/2020 19:03:06 - INFO - __main__ -   Mean top-1 acc: 59.00

Train Epoch: 35/ 256. Iter: 1024/1024. LR: 0.0589. Loss: 4.9775. Loss_x: 1.9495. Loss_u: 0.0452. Loss_s: 2.9829. Mask: 0.19. : 100%|█████████████████████| 1024/1024 [03:29<00:00,  4.90it/s]
Test Iter:  527/ 527. Data: 0.034s. Batch: 0.056s. Loss: 1.5550. top1: 60.61. top5: 84.75. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:29<00:00, 17.98it/s]
11/09/2020 19:07:04 - INFO - __main__ -   top-1 acc: 60.61
11/09/2020 19:07:04 - INFO - __main__ -   top-5 acc: 84.75
11/09/2020 19:07:05 - INFO - __main__ -   Best top-1 acc: 60.61
11/09/2020 19:07:05 - INFO - __main__ -   Mean top-1 acc: 59.24

Train Epoch: 36/ 256. Iter: 1024/1024. LR: 0.0589. Loss: 4.9592. Loss_x: 1.9311. Loss_u: 0.0461. Loss_s: 2.9820. Mask: 0.10. : 100%|█████████████████████| 1024/1024 [03:29<00:00,  4.89it/s]
Test Iter:  527/ 527. Data: 0.035s. Batch: 0.056s. Loss: 1.5581. top1: 60.62. top5: 84.58. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:29<00:00, 17.87it/s]
11/09/2020 19:11:03 - INFO - __main__ -   top-1 acc: 60.62
11/09/2020 19:11:03 - INFO - __main__ -   top-5 acc: 84.58
11/09/2020 19:11:03 - INFO - __main__ -   Best top-1 acc: 60.62
11/09/2020 19:11:03 - INFO - __main__ -   Mean top-1 acc: 59.44

Train Epoch: 37/ 256. Iter: 1024/1024. LR: 0.0588. Loss: 4.9411. Loss_x: 1.9131. Loss_u: 0.0469. Loss_s: 2.9812. Mask: 0.19. : 100%|█████████████████████| 1024/1024 [03:28<00:00,  4.91it/s]
Test Iter:  527/ 527. Data: 0.034s. Batch: 0.055s. Loss: 1.5628. top1: 60.73. top5: 84.75. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:29<00:00, 18.09it/s]
11/09/2020 19:15:01 - INFO - __main__ -   top-1 acc: 60.73
11/09/2020 19:15:01 - INFO - __main__ -   top-5 acc: 84.75
11/09/2020 19:15:01 - INFO - __main__ -   Best top-1 acc: 60.73
11/09/2020 19:15:01 - INFO - __main__ -   Mean top-1 acc: 59.61

Train Epoch: 38/ 256. Iter: 1024/1024. LR: 0.0588. Loss: 4.9239. Loss_x: 1.8959. Loss_u: 0.0477. Loss_s: 2.9804. Mask: 0.21. : 100%|█████████████████████| 1024/1024 [03:31<00:00,  4.85it/s]
Test Iter:  527/ 527. Data: 0.036s. Batch: 0.057s. Loss: 1.5647. top1: 60.78. top5: 84.60. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:30<00:00, 17.42it/s]
11/09/2020 19:19:03 - INFO - __main__ -   top-1 acc: 60.78
11/09/2020 19:19:03 - INFO - __main__ -   top-5 acc: 84.60
11/09/2020 19:19:03 - INFO - __main__ -   Best top-1 acc: 60.78
11/09/2020 19:19:03 - INFO - __main__ -   Mean top-1 acc: 59.77

Train Epoch: 39/ 256. Iter: 1024/1024. LR: 0.0587. Loss: 4.9073. Loss_x: 1.8791. Loss_u: 0.0485. Loss_s: 2.9797. Mask: 0.31. : 100%|█████████████████████| 1024/1024 [03:33<00:00,  4.80it/s]
Test Iter:  527/ 527. Data: 0.035s. Batch: 0.056s. Loss: 1.5647. top1: 60.69. top5: 84.79. : 100%|█████████████████████████████████████████████████████████| 527/527 [00:29<00:00, 17.89it/s]
11/09/2020 19:23:05 - INFO - __main__ -   top-1 acc: 60.69
11/09/2020 19:23:05 - INFO - __main__ -   top-5 acc: 84.79
11/09/2020 19:23:05 - INFO - __main__ -   Best top-1 acc: 60.78
11/09/2020 19:23:05 - INFO - __main__ -   Mean top-1 acc: 59.90
 